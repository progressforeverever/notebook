---
title: 爬虫基础
date: 2023-06-26 16:53:01
permalink: /pages/6c74eb/
categories:
  - python
  - 爬虫
tags:
  - 
---

通过学习爬虫，也可以学习到其他的很多附带的东西，也相当于一个小项目、小工具。

### 爬虫基础

#### 快速上手

- 爬虫的几个步骤
  - 指定url
  - 发起请求
  - 获取响应数据
  - 持久化存储

```python
# 指定url
# 发起请求
# 获取响应数据
# 持久化存储

import requests

#指定url
url='https://www.baidu.com/'

#发起请求   使用request模块
response=requests.get(url=url)
#get方法返回一个响应对象

#获取响应数据

page_text=response.text     #字符串形式的数据（HTML的数据）
print(page_text)

#持久化存储 可以存到一个文件中

with open('./baidu.html','w',encoding='utf-8') as f:
    f.write(page_text)

print('爬取结束')


```

#### 善于利用F12观察

![image-20230321093914064](https://czynotebook.oss-cn-beijing.aliyuncs.com/spider01.png)

- fetch/XHR  看一些Ajax请求的数据
- 主要看请求头  里面会包含很多响应返回的数据
- 预览和响应也要关注

```python
'''
请求 URL: https://movie.douban.com/j/chart/top_list?type=2&interval_id=100%3A90&action=&start=109&limit=20
请求方法: GET
Keep-Alive: timeout=30
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49
Content-Type: application/json; charset=utf-8

#带的参数   url问号后面那一块
type: 2
interval_id: 100:90
action: 
start: 109  #猜测数据的意思：从库中第几部电影开始取
limit: 20   #一次取几部
'''

from urllib import response
import requests
import json
get_url='https://movie.douban.com/j/chart/top_list?'
headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49'}
data={'type':2,'interval_id':'100:90','action':'','start':'1','limit':'20'}

response=requests.get(url=get_url,params=data,headers=headers)
dict_obj=response.json()
print(dict_obj)
fp=open('./douban.json','w',encoding='utf-8')
json.dump(dict_obj,fp=fp,ensure_ascii=False)

print('爬取结束')


```



#### 百度翻译的爬取

破解百度翻译的爬虫

第一部分  **分析爬取什么，爬取对象的特点**

  1.首先，百度翻译有个特点，输入后会自动刷新，**Ajax技术（不用点击刷新页面，就能使网络自动刷新，异步编程）**

  2.第二，我们爬取的是部分内容了，不再是像以前那样的爬取整个网页了，那么首先，url怎么看呢？

  3.因为是Ajax请求**（爬的时候要根据特点判别出来）**，所以打开开发者工具（F12），进入XHR板块查看（针对与Ajax请求）

  4.然后就是分析网页的数据，我么要看的几个内容是：

​      请求 URL: https://fanyi.baidu.com/sug

​      请求方法: POST

​      Connection: keep-alive

​      User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49

​      **kw: dog(Edge浏览器在负载中看关键词)**



  5.我们**要关注返回的数据类型**

​    Content-Type: application/json

​    返回的数据类型是json类型，所以有相应的处理方式

以上是前期准备

- 这里还涉及了**json文件的处理**

```python
from fileinput import filename
from os import system
import requests
import json

post_url='https://fanyi.baidu.com/sug'
#UA伪装
UAPretend={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49'}
#有参数的话处理参数
value=input('请输入想要翻译的词语：')
data={'kw':value}
#发起请求
response=requests.post(url=post_url,data=data,headers=UAPretend)

#post请求同get请求三个参数（url,需要处理的参数（封装成字典形式），头信息（UA伪装）（字典形式））

#获取响应数据处理数据
dic_obj=response.json()
#json方法返回的是obj,(只有返回的是json数据类型才能用json方法)
print(dic_obj)

#持久化存储
fileName=value+'.json'
fp=open(fileName,'w',encoding='utf-8')
json.dump(dic_obj,fp=fp,ensure_ascii=False)     #json文件处理三个参数（对象，文件句柄，ASCII码处理方式），这里是False因为是里面有中文，中文不能用ASCII表示

import os
os.system('pause')
a=input()
#print('爬取结束')

```



#### 药监局的爬取（url的确定）

什么是Ajax动态加载？

**直接简单定义在response中搜索不到？**

```
'''
药监局需求分析：
目标    拿到药监局的详情页的数据
已知：  http://scxk.nmpa.gov.cn:81/xk/  药监局网址
观察首页，第一个思路，我们爬取知道的网址，里面有企业名称，许可证编号等，通过a标签及其href属性来提取
那么，一个问题：能否提取到？
验证方法：法一：用代码去爬取一下，来看一下
        法二：打开开发者工具，里面的response(响应)即是我们要爬取的内容，在里面用搜索（更快）

结果发现：直接爬取网页并不能爬取到
那么有可能，网页是通过动态加载的
转到XHR来看一下：
        #
            请求方法: POST
            Content-Type: application/json;charset=UTF-8
            on: true
            带的参数
            page: 1
            pageSize: 15
            productName: 
            conditionType: 1
            applyname: 
            applysn: 

            不要忘了看response数据
            将json数据格式化校验
            观察json串：
                    并没有url，
                    但发现每个企业都对应一个id
                    id往往是比较重要的一部分
            从首页粘几个详情页的url:(这个就是找几个例子，去挖掘他们的共同点)
                    http://scxk.nmpa.gov.cn:81/xk/itownet/portal/dzpz.jsp?id=fb41790cea4e419b957d1e4f63c16b63
                    http://scxk.nmpa.gov.cn:81/xk/itownet/portal/dzpz.jsp?id=8b8be35cd5824b9888c11affc27ef273

                    发现：前面的域名一样，后面的id不一样    后面的id是参数
                    后面的id是否是json串里面的id?
                    "ID": "fb41790cea4e419b957d1e4f63c16b63",
		            "EPS_NAME": "苏州市珊亚生物科技有限公司",
                    结论：一样的

                    所以，我们可以从首页获取各个详情页的id，再和域名拼接，就可以得到详情页的url

                    还是那个问题，我知道了url，又能否提取到呢?万一又是动态加载呢?

                    再次去查看：
                        在运行的ALL中，进行查找，发现找不到。
                        再去看XHR，发现有，说明详情页也是个动态加载

                        查看XHR：
                            id: fb41790cea4e419b957d1e4f63c16b63(带的参数)(可以在首页的json数据中获取)
                            Content-Type: application/json;charset=UTF-8
                            请求 URL: http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?hKHnQfLv=5jw09RBzh0u_EDQb0z3cNOr5AWPhCjzYlD.K1OdCQlKlc3VzCgBGgwCvOAW4ZP4PdlZewr2ey4XlvnmWQ8udaSJJki62iXVFT7A7hsULcYq3Z02vHr.wxOBU63E9EBx3D5wh.YGd3.71O.TW1LLE_sc3ul6jmHQu_lirB27GBiOoCekY9frjHrEcHaW5e1MUOpcxyiWjljc_FDUOeEAzYZusZME.SkK6CRyF92ON7XjvYhk50aUL8f4Oqdrf17rxk0Ssqf4_NJFvYL5jb6M1dgcp5ayKfrl0clkw3op1pznq&8X7Yi61c=4i929QTgJ1wIvSylDlHquV.MhO3LbR6Lftcw_ue4PHYJK6lnFYDjsFQ8J.4S_3TdqnP24NO9V5TggF67MMsvhbMN3Cg6p08t9P9owDVDpsGGwyhl1FUwmmDEg4RxeDavF
                            看看response!!!有我们想要找的信息

                            然后发现，请求的URL除了携带的参数(id)不一样，前面的域名是一样的，那这样就可以了
        #

        我们希望在首页获取各个详情页的url
核心在于URL的确定
'''

```

**python爬虫的一大特性就在于其灵活、需要自己去观察  观察  找规律**


#### UA伪装 参数处理

```python
from email import header
from fileinput import filename
import requests
url='https://www.sogou.com/web?'    #基础的url

value=input('请输入想查询的内容:')
para={'query':value}
#应用requests.get方法中的params参数，目的是让自己的url不再那么臭长，使自己的代码看起来更加整齐
#要封装成字典形式
'''
params：字典或者字节序列，作为参数增加到url中

例子：

import requests
kv={“wd”:“你好”}#拼接的内容用字典储存
r=requests.request(“GET”,“http://www.baidu.com/s”,params=kv)
print(r.url)
print(r.text)
运行后拼接的效果：http://www.baidu.com/s?wd=你好

'''

#对指定的url发起请求，是带参数的
#反反爬策略   UA伪装
#浏览器的反扒措施：浏览器会通过user-agent来判断用户是何种方式访问的，若是浏览器，则允许访问，若不是，有可能会拒绝
#因此，需要让自己的访问看起来像是浏览器访问的，就伪装自己的user-agent

#UA伪装，也要封装成字典形式
headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49'}
#头信息，伪装成了EDGE浏览器
response=requests.get(url=url,params=para,headers=headers)
page_txt=response.text
print(page_txt)

fileName=value+'.html'
with open(fileName,'w',encoding='utf-8') as f:
    f.write(page_txt)

print('爬取完毕')
```


### 数据解析

#### 概述

拿到网页页面后，就应该进行数据解析

数据解析分类：

-   正则（解析一些特定要求的内容）
-   bs4
-   Xpath（这个用的最多）



数据解析原理：

  局部的文本会存储在标签或者标签中的属性之中

  所以：

​    **1.对标签进行定位**

​    **2.对内容进行提取**



所以爬取的步骤变为：

  指定url

  发起请求

  获取响应数据     **text 返回字符串   content  返回二进制存储   json()  返回对象**

  解析数据

  持久化存储



bs4数据解析：

  1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到BeautifulSoup对象中

  

```python
  fp=open('./sougou.html','r',encoding='utf-8')

​    \#第二个参数基本上固定了，是lxml解析器

​    soup=BeautifulSoup(fp,'lxml')  

​    print(soup)#发现打印出来的是页面的源码
```



  2.调用BeautifulSoup对象中的相关属性和方法进行标签定位和数据解析

```python
    page_text=response.text

   soup=BeautifulSoup(page_text,'lxml')
```





**Xpath解析原理：**

 

```
 from lxml import etree
```

  1.实例化一个etree对象，并且将需要解析的页面数据加载到对象中

  2.调用etree对象中的Xpath方法和Xpath表达式对标签进行定位

  实例化对象：

```
1.将本地的html文件加载：etree.parse(filepath)

​    2.将互联网的页面加载：etree.HTML('page_text')
```

  -Xpath('Xpath表达式')

  所以**核心是Xpath表达式**





#### 正则

> 限定符(作用对象是一个字符):
>
> ? 匹配0个或一个    used?  对d字符(作用在限定符的前面) 可匹配:use used
>
> \* 匹配0个或多个   ab*c   匹配ac  abc abbbbbc
>
> \+ 匹配1个以上的   ab+c   abc abbbbbc
>
> {}表示范围      ab{2}c    abbc   ab{2,6}c   b可出现2-6次   ab{2,}c b可出现2次以上
>
> 
>
> 或运算符：  |
>
> a (cat|dog) 括号不可少
>
> 
>
> 字符类：
>
> [abc]+  由a、b、c[中括号内的字符]组成的字符
>
> [a-zA-Z0-9]+
>
> [^0-9]+   ^匹配除了后面内容以外的东东
>
> 
>
> 元字符：
>
> /d  数字   digit  \D  非数字
>
> /w  单词       \w非单词
>
> ^  匹配行首   $匹配行尾  ^a 只匹配开头有a的字符    $d 只匹配结尾有d的字符
>
> 
>
> .匹配任意字符    所以匹配"."要用注意字符\     \.  匹配". "   \d+\. \d+\. \d+\. \d+\.   匹配IPV地址
>
> 正则表达式默认贪婪匹配
>
> <.+?>  ?改成懒惰匹配  常用来匹配标签

```python
import re
str1='abc123*+-'
str_find=re.findall('c123',str1)
print(str_find)

a=re.match('test','testasdtest')
print(a)                #<re.Match object; span=(0, 4), match='test'>
print(a.group())        #取到了就是结果
print(a.span())         #区间，左闭右开
print(re.match('test','atestasdtest'))  #返回None

'''
re.match()方法返回一个匹配的对象，而不是匹配的内容。
如果需要返回内容则需要调用group()。
通过调用span()可以获得匹配结果的位置。
而如果从起始位置开始没有匹配成功，即便其他部分包含需要匹配的内容,re.match()也会返回None.
(1)findall()
找到re匹配的所有字符串,返回一个列表
(2)search()
扫描字符串,找到这个re匹配的位置(仅仅是第一个查到的)
(3)match()
决定re是否在字符串刚开始的位置(匹配行首)
'''

import re
print(re.sub('php','python','php是世界上最好的语言——php'))  
#输出 "python是世界上最好的语言——python"

import re
print(re.match(f"aa\d+","aa2323"))   #会尽可能多的去匹配\d
print(re.match(f"aa\d+?","aa2323"))  #尽可能少的去匹配\d
```



#### Xpath

```python
from lxml import etree



if __name__ == '__main__':
    tree=etree.parse('sougou.html') #实例化etree对象
    r=tree.xpath('/html/body/div')      #根据层级关系进行定位  返回的是element类型的对象
    #r=tree.xpath('/html//div')     //表示多个层级  可以表示从任意位置开始定位
    # /最左侧的是根节点 
    #r=tree.xpath('//div[@class="song"]')   [@class=' xxx' ]        属性定位
    #r=tree.xpath('//div[@class="song"]/p[3]')      索引定位  索引从1开始

    #取文本、取属性
    #/text() 取文本     返回的是直系文本的内容      //text()获取的是标签中非直系文本的内容（也就是所有的文本内容）
    #r=tree.xpath('//div[@class="song"]/p[3]/text()')       返回的是列表
    #r=tree.xpath('//div[@class="song"]/img/@src)       /@src   取属性
    print(r)
```



实例

```python
import requests
from lxml import etree

if __name__ == '__main__':
    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63'}
    url='https://www.aqistudy.cn/historydata/'

    response=requests.get(url=url,headers=headers)
    page_text=response.text
    #print(page_text)

    tree=etree.HTML(page_text)
    hot_city=tree.xpath("//div[@class='hot']//li/a/text()")
    print(hot_city)
    total_city=tree.xpath("//div[@class='all']//li/a/text()")
    print(total_city)
```



#### selenium

1.如何发现是动态加载的?

  **在network下找到与自己url相对应的页面**

  在页面中搜索一些页面中的信息，**若没有搜到，极为动态加载**

2.之前怎么处理的呢?

  在全部文件下进行全局搜索，找到动态加载的页面对应的文件

  在文件中查看详细信息，会找到真正的url,发起请求即可

selenium模块：

  1.获取动态加载的数据

  2.实现模拟登录

**selenium是一个基于浏览器自动化的模块**

1.实例化对象

2.编写程序进行自动化操作



**Xpath解析原理：**

 

```
 from lxml import etree
```

  1.实例化一个etree对象，并且将需要解析的页面数据加载到对象中

  2.调用etree对象中的Xpath方法和Xpath表达式对标签进行定位

  实例化对象：

```
1.将本地的html文件加载：etree.parse(filepath)

​    2.将互联网的页面加载：etree.HTML('page_text')
```

  -Xpath('Xpath表达式')

  所以**核心是Xpath表达式**





#### 正则

> 限定符(作用对象是一个字符):
>
> ? 匹配0个或一个    used?  对d字符(作用在限定符的前面) 可匹配:use used
>
> \* 匹配0个或多个   ab*c   匹配ac  abc abbbbbc
>
> \+ 匹配1个以上的   ab+c   abc abbbbbc
>
> {}表示范围      ab{2}c    abbc   ab{2,6}c   b可出现2-6次   ab{2,}c b可出现2次以上
>
> 
>
> 或运算符：  |
>
> a (cat|dog) 括号不可少
>
> 
>
> 字符类：
>
> [abc]+  由a、b、c[中括号内的字符]组成的字符
>
> [a-zA-Z0-9]+
>
> [^0-9]+   ^匹配除了后面内容以外的东东
>
> 
>
> 元字符：
>
> /d  数字   digit  \D  非数字
>
> /w  单词       \w非单词
>
> ^  匹配行首   $匹配行尾  ^a 只匹配开头有a的字符    $d 只匹配结尾有d的字符
>
> 
>
> .匹配任意字符    所以匹配"."要用注意字符\     \.  匹配". "   \d+\. \d+\. \d+\. \d+\.   匹配IPV地址
>
> 正则表达式默认贪婪匹配
>
> <.+?>  ?改成懒惰匹配  常用来匹配标签

```python
import re
str1='abc123*+-'
str_find=re.findall('c123',str1)
print(str_find)

a=re.match('test','testasdtest')
print(a)                #<re.Match object; span=(0, 4), match='test'>
print(a.group())        #取到了就是结果
print(a.span())         #区间，左闭右开
print(re.match('test','atestasdtest'))  #返回None

'''
re.match()方法返回一个匹配的对象，而不是匹配的内容。
如果需要返回内容则需要调用group()。
通过调用span()可以获得匹配结果的位置。
而如果从起始位置开始没有匹配成功，即便其他部分包含需要匹配的内容,re.match()也会返回None.
(1)findall()
找到re匹配的所有字符串,返回一个列表
(2)search()
扫描字符串,找到这个re匹配的位置(仅仅是第一个查到的)
(3)match()
决定re是否在字符串刚开始的位置(匹配行首)
'''

import re
print(re.sub('php','python','php是世界上最好的语言——php'))  
#输出 "python是世界上最好的语言——python"

import re
print(re.match(f"aa\d+","aa2323"))   #会尽可能多的去匹配\d
print(re.match(f"aa\d+?","aa2323"))  #尽可能少的去匹配\d
```



#### Xpath

```python
from lxml import etree



if __name__ == '__main__':
    tree=etree.parse('sougou.html') #实例化etree对象
    r=tree.xpath('/html/body/div')      #根据层级关系进行定位  返回的是element类型的对象
    #r=tree.xpath('/html//div')     //表示多个层级  可以表示从任意位置开始定位
    # /最左侧的是根节点 
    #r=tree.xpath('//div[@class="song"]')   [@class=' xxx' ]        属性定位
    #r=tree.xpath('//div[@class="song"]/p[3]')      索引定位  索引从1开始

    #取文本、取属性
    #/text() 取文本     返回的是直系文本的内容      //text()获取的是标签中非直系文本的内容（也就是所有的文本内容）
    #r=tree.xpath('//div[@class="song"]/p[3]/text()')       返回的是列表
    #r=tree.xpath('//div[@class="song"]/img/@src)       /@src   取属性
    print(r)
```



实例

```python
import requests
from lxml import etree

if __name__ == '__main__':
    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63'}
    url='https://www.aqistudy.cn/historydata/'

    response=requests.get(url=url,headers=headers)
    page_text=response.text
    #print(page_text)

    tree=etree.HTML(page_text)
    hot_city=tree.xpath("//div[@class='hot']//li/a/text()")
    print(hot_city)
    total_city=tree.xpath("//div[@class='all']//li/a/text()")
    print(total_city)
```



#### selenium

1.如何发现是动态加载的?

  **在network下找到与自己url相对应的页面**

  在页面中搜索一些页面中的信息，**若没有搜到，极为动态加载**

2.之前怎么处理的呢?

  在全部文件下进行全局搜索，找到动态加载的页面对应的文件

  在文件中查看详细信息，会找到真正的url,发起请求即可

selenium模块：

  1.获取动态加载的数据

  2.实现模拟登录

**selenium是一个基于浏览器自动化的模块**

1.实例化对象

2.编写程序进行自动化操作



**然而playwright的自动生成代码更加好使**

##### 规避检测

```python
from lib2to3.pgen2 import driver
from selenium import webdriver
from time import sleep

#规避检测
from selenium.webdriver import ChromeOptions


#实现规避检测
option=ChromeOptions()
option.add_experimental_option('excludeSwitches',['enable-automation'])
driver=webdriver.Chrome(executable_path='chromedriver.exe',options=option)
```

##### 自动化操作

```python
from lib2to3.pgen2 import driver
from selenium import webdriver
from selenium.webdriver.common.by import By
from time import sleep
option = webdriver.ChromeOptions()
# 防止打印一些无用的日志
option.add_experimental_option("excludeSwitches", ['enable-automation', 'enable-logging'])
driver=webdriver.Chrome(executable_path='chromedriver.exe',options=option)

driver.get("https://www.taobao.com/")

#像搜索框录入一个词
#1.定位搜索框
#因为它是唯一的，所以一般可以使用find_element()来定位
search_input=driver.find_element(By.ID,value='q')
'''（1）find_element()的返回结果是一个WebElement对象，如果符合条件的有多个，默认返回找到的第一个，如果没有找到则抛出NoSuchElementException异常。

（2）find_elements()的返回结果是一个包含所有符合条件的WebElement对象的列表，如果未找到，则返回一个空列表。
'''
#search_input=driver.find_element_by_id("q")
#2.标签交互
search_input.send_keys("iphone")

#3.点击按钮
search_button=driver.find_element(By.CSS_SELECTOR,value='.btn-search')
search_button.click()

sleep(5)
driver.quit()
```



**selenium还需要用webdriver.exe 这就很不方便**



##### edgeDriver

```python
from selenium import webdriver

driver=webdriver.Edge(executable_path='msedgedriver.exe')
driver.get("https://www.csdn.net/")
page_text=driver.page_source
print(page_text)
```



##### iframe动作链

```python
from selenium import webdriver
from time import sleep
from selenium.webdriver.common.by import By
from selenium.webdriver import ActionChains

option = webdriver.ChromeOptions()
# 防止打印一些无用的日志
option.add_experimental_option("excludeSwitches", ['enable-automation', 'enable-logging'])
driver=webdriver.Chrome(executable_path='chromedriver.exe',options=option)
driver.get('https:www.runoob.com/try/try.php?filename=jqueryui-api-droppable')

#进行拖动操作
#1.标签定位
driver.switch_to.frame('iframeResult')      #切换浏览器的作用域 切到
drag_div=driver.find_element(By.ID,value='draggable')
#selenium.common.exceptions.NoSuchElementException:
#  Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="draggable"]"}

#iframe  嵌套页面  直接find是定位不到的
#print(drag_div)

#拖动操作  实际上是一系列的动作 导入动作链的包
#实例化动作链对象

action=ActionChains(driver)

#点击长按指定的
action.click_and_hold(drag_div).perform()
#移动偏移像素

for i in range(5):
    #(x,y) x左右拖动    y数值拖动
    action.move_by_offset(50,0).perform()           #只有perform后才会执行
    sleep(0.3)

action.release().perform()

driver.quit()
```



### 遇到的一些反爬

#### noscript

noscript 元素用来定义在脚本未被执行时的替代内容（文本）。

此标签可被用于可识别 noscript 标签但无法支持其中的脚本的浏览器。



#### iframe

```
<iframe> 标签规定一个内联框架。
```

一个内联框架被用来在当前 HTML 文档中嵌入另一个文档

貌似要用selenium啥的 



#### F12调试工具被禁用

在爬取一些网页的时候F12快捷键不能使用，不能打开调试界面
解决办法：以谷歌浏览器为例，左上角选项—更多工具–开发者工具
或者使用ctrl+shift+I快捷键打开
但是打开调试界面后，网页界面卡住了，调试界面sources中不停的循环一个debugger函数这是因为网页添加的js代码进行了过禁用调试设置，不停的循环这个debugger函数，几次之后就会内存增长造成网页卡顿



解决办法：
一、禁用调试模式

这样可以正常显示，但是我们不能正常调试其他代码了
二、在循环提示debugger的地方，在3的地方右击，选择添加有条件断点（Add conditional breakpoint），设置条件为false

![img](https://czynotebook.oss-cn-beijing.aliyuncs.com/spider03.png)

![img](https://czynotebook.oss-cn-beijing.aliyuncs.com/spider04.png)



这样只能解决部分js代码，对于有一些禁用调试的js代码还是无用的
与之类似的选择Never pause here 也可以解决部分js禁用，也有不能解决的情况
三、未验证方法，提供思路
1.Chrome有一个Local Override 功能
2.使用抓包工具fiddler抓到这个js，返回的内容改成去除debuger相关代码

或者换换浏览器  貌似Edge浏览器可以解决



### playwright

[新兴爬虫利器 Playwright 的基本用法 | 静觅 (cuiqingcai.com)](https://cuiqingcai.com/36045.html)

自动代码生成

```
playwright codegen -o script.py -b firefox
```

比如 **-o 代表输出的代码文件的名称**；**—target 代表使用的语言**，默认是 python，即会生成同步模式的操作代码，如果传入 python-async 就会生成异步模式的代码；**-b 代表的是使用的浏览器，默认是 Chromium**，


